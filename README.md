# Deep Learning Course Notebooks  
*A collection of notebooks implementing fundamental deep learning concepts from scratch.*

## 📌 Overview  
This repository contains **Jupyter notebooks** created as part of a deep learning course. Each notebook focuses on a specific topic, implementing key deep learning models **without relying on high-level libraries like TensorFlow or PyTorch**, to reinforce a deep understanding of the underlying mechanics.

New notebooks will be added over time, covering various aspects of deep learning, from foundational architectures to optimization techniques.

---

## 📁 Notebooks  


### ** 1️⃣Micrograd: Building an Autograd Engine from Scratch**  
📌 *[Micrograd.ipynb](Micrograd.ipynb)*  

✅ Recreates a **simple autograd engine**, inspired by `micrograd` (by Karpathy), to understand how backpropagation works.  
✅ Builds a **computational graph**, implementing **automatic differentiation** to compute gradients.  
✅ Serves as the foundation for understanding how deep learning frameworks like TensorFlow and PyTorch handle gradients internally.  

**Key Concepts Covered:**  
- Computational graphs  
- Forward and backward passes  
- Automatic differentiation  

---

### ** 2️⃣Bigram Model**  
📌 *[Bigram model.ipynb](Bigram model.ipynb)*  

✅ Implements a **bigram language model**, where words are predicted based on the previous token.  
✅ Uses **count-based probability estimation** to generate realistic text sequences.  
✅ Demonstrates **N-gram modeling** as a simple yet powerful NLP technique.  

**Key Concepts Covered:**  
- Probability distributions in NLP  
- Tokenization and frequency analysis  
- Bigram-based text generation  

---

