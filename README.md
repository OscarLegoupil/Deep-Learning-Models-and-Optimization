# Deep Learning Course Notebooks  
*A collection of notebooks implementing fundamental deep learning concepts from scratch.*

## ğŸ“Œ Overview  
This repository contains **Jupyter notebooks** created as part of a deep learning course. Each notebook focuses on a specific topic, implementing key deep learning models **without relying on high-level libraries like TensorFlow or PyTorch**, to reinforce a deep understanding of the underlying mechanics.

New notebooks will be added over time, covering various aspects of deep learning, from foundational architectures to optimization techniques.

---

## ğŸ“ Notebooks  


### ** 1ï¸âƒ£Micrograd: Building an Autograd Engine from Scratch**  
ğŸ“Œ *[Micrograd.ipynb](Micrograd.ipynb)*  

âœ… Recreates a **simple autograd engine**, inspired by `micrograd` (by Karpathy), to understand how backpropagation works.  
âœ… Builds a **computational graph**, implementing **automatic differentiation** to compute gradients.  
âœ… Serves as the foundation for understanding how deep learning frameworks like TensorFlow and PyTorch handle gradients internally.  

**Key Concepts Covered:**  
- Computational graphs  
- Forward and backward passes  
- Automatic differentiation  

---

### ** 2ï¸âƒ£Bigram Model**  
ğŸ“Œ *[Bigram model.ipynb](Bigram model.ipynb)*  

âœ… Implements a **bigram language model**, where words are predicted based on the previous token.  
âœ… Uses **count-based probability estimation** to generate realistic text sequences.  
âœ… Demonstrates **N-gram modeling** as a simple yet powerful NLP technique.  

**Key Concepts Covered:**  
- Probability distributions in NLP  
- Tokenization and frequency analysis  
- Bigram-based text generation  

---

